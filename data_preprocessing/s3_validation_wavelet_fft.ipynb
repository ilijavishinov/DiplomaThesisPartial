{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json_module, dirs_module\n",
    "import txt_module\n",
    "import importlib\n",
    "from frequency_domain_module import wavelet_transform, fft_transform, histogram_sums\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# dirs and files\n",
    "project_dir = 'D:/FINKI/40_diploma_thesis'\n",
    "\n",
    "healthy_metadata = pd.read_csv(f'{project_dir}/metadata/dataset_paper_tables/DatasetPaper_Table7.csv')\n",
    "real_damage_metadata = pd.read_csv(f'{project_dir}/metadata/dataset_paper_tables/DatasetPaper_Table5.csv')\n",
    "artificial_damage_metadata = pd.read_csv(f'{project_dir}/metadata/dataset_paper_tables/DatasetPaper_Table4.csv')\n",
    "y_map = json_module.read_json(f'{project_dir}/metadata/y_map.json')\n",
    "\n",
    "data_dir = r'E:\\40_diploma_thesis\\datasets_numpy_1S_OffsetHealthy001_OffsetDamaged005'\n",
    "testing_data_dir = f'F:\\s3_test_datasets_numpy_1S_OffsetHealthy001_OffsetDamaged005'\n",
    "write_data_dir = rf'G:/data/splits/s3_1s_bayesian'\n",
    "dirs_module.create_directory(write_data_dir)\n",
    "\n",
    "healthy_train = ['K001','K002','K003']\n",
    "real_damage_train = ['KA04','KA15','KA22','KA30','KB23','KB27','KI04','KI17']\n",
    "artificial_damage_train = ['KA01','KA05','KA07','KI01','KI03']\n",
    "train_bearing_codes = healthy_train + artificial_damage_train + real_damage_train\n",
    "\n",
    "healthy_val = ['K004']\n",
    "real_damage_val = ['KB27', 'KI14']\n",
    "artificial_damage_val = ['KI05']\n",
    "validation_bearing_codes = healthy_val + artificial_damage_val + real_damage_val\n",
    "\n",
    "txt_module.list_to_txt(train_bearing_codes, f'{write_data_dir}/train_bearing_codes.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 3 N09_M07_F10\n",
      "700 3 N15_M07_F10\n",
      "700 3 N15_M01_F10\n",
      "700 3 N15_M07_F04\n"
     ]
    }
   ],
   "source": [
    "def include_file_in_train(file_name_, regime_):\n",
    "    for train_bearing_code in train_bearing_codes:\n",
    "        if train_bearing_code in file_name_:\n",
    "            if regime_ in file_name_:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def include_file_in_test(file_name_, regime_):\n",
    "    bearing_code = file_name_.split('_')[3]\n",
    "    if bearing_code in train_bearing_codes:\n",
    "        return False\n",
    "    if bearing_code in validation_bearing_codes:\n",
    "        return False\n",
    "    if regime_ not in file_name_:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def include_file_in_val(file_name_, regime_):\n",
    "    bearing_code = file_name_.split('_')[3]\n",
    "    if bearing_code in train_bearing_codes:\n",
    "        return False\n",
    "    if bearing_code not in validation_bearing_codes:\n",
    "        return False\n",
    "    if regime_ not in file_name_:\n",
    "        return False\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for bin_width in [700]:\n",
    "    for levels in [3]:\n",
    "        for regime in ['N09_M07_F10', 'N15_M07_F10', 'N15_M01_F10', 'N15_M07_F04']:\n",
    "            print(bin_width, levels, regime)\n",
    "\n",
    "            train_files = [file for file in os.listdir(data_dir)\n",
    "                           if include_file_in_train(file, regime)]\n",
    "            test_files = [file for file in os.listdir(testing_data_dir)\n",
    "                           if include_file_in_test(file, regime)]\n",
    "            val_files = [file for file in os.listdir(testing_data_dir)\n",
    "                           if include_file_in_val(file, regime)]\n",
    "\n",
    "            for set_type, set_type_files in zip(['train', 'test', 'val'], [train_files, test_files, val_files]):\n",
    "\n",
    "                file_names = list()\n",
    "                y = list()\n",
    "                for file in set_type_files:\n",
    "                    bearing_code = file.split('_')[3]\n",
    "                    file_names.append(file.strip('.npy'))\n",
    "                    y.append(y_map[bearing_code])\n",
    "                np.save(f'{write_data_dir}/y_{set_type}_{regime}_{levels}_{bin_width}.npy',y)\n",
    "                txt_module.list_to_txt(file_names, f'{write_data_dir}/x_{set_type}_index.txt')\n",
    "\n",
    "                arrays = list()\n",
    "                for file in set_type_files:\n",
    "                    if set_type != 'train':\n",
    "                        if file.split('_')[3] in train_bearing_codes:\n",
    "                            continue\n",
    "                        feature_array = np.load(f'{data_dir}/{file}', allow_pickle = True).astype('float32').ravel()\n",
    "                    else:\n",
    "                        feature_array = np.load(f'{testing_data_dir}/{file}', allow_pickle = True).astype('float32').ravel()\n",
    "                    feature_array = np.array(wavelet_transform(feature_array, levels_ = levels, bin_width_ = bin_width), dtype = 'float32')\n",
    "                    arrays.append(feature_array)\n",
    "\n",
    "                for i in range(len(arrays)):\n",
    "                    arrays[i] = np.reshape(arrays[i], (1, arrays[i].shape[0]))\n",
    "                X = np.concatenate(arrays, axis = 0); del arrays\n",
    "                scaler = StandardScaler()\n",
    "                X = scaler.fit_transform(X)\n",
    "                np.save(f'{write_data_dir}/x_{set_type}_{regime}_{levels}_{bin_width}.npy', X); del X\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 1 N09_M07_F10\n",
      "15 1 N15_M07_F10\n",
      "15 1 N15_M01_F10\n",
      "15 1 N15_M07_F04\n"
     ]
    }
   ],
   "source": [
    "NUM_COEF_TO_SAVE = 12500\n",
    "pairwise_bin_width_zero_padding_multiple =  [\n",
    "    (15, 1),\n",
    "]\n",
    "\n",
    "for bin_width, zero_padding_multiple in pairwise_bin_width_zero_padding_multiple:\n",
    "        for regime in ['N09_M07_F10', 'N15_M07_F10', 'N15_M01_F10', 'N15_M07_F04']:\n",
    "            print(bin_width, zero_padding_multiple, regime)\n",
    "\n",
    "            train_files = [file for file in os.listdir(data_dir)\n",
    "                           if include_file_in_train(file, regime)]\n",
    "            test_files = [file for file in os.listdir(testing_data_dir)\n",
    "                           if include_file_in_test(file, regime)]\n",
    "            val_files = [file for file in os.listdir(testing_data_dir)\n",
    "                           if include_file_in_val(file, regime)]\n",
    "\n",
    "            for set_type, set_type_files in zip(['train', 'test', 'val'], [train_files, test_files, val_files]):\n",
    "\n",
    "                file_names = list()\n",
    "                y = list()\n",
    "                for file in set_type_files:\n",
    "                    bearing_code = file.split('_')[3]\n",
    "                    file_names.append(file.strip('.npy'))\n",
    "                    y.append(y_map[bearing_code])\n",
    "                np.save(f'{write_data_dir}/y_{set_type}_{regime}_{levels}_{bin_width}.npy',y)\n",
    "                txt_module.list_to_txt(file_names, f'{write_data_dir}/x_{set_type}_index.txt')\n",
    "\n",
    "                arrays = list()\n",
    "                for file in set_type_files:\n",
    "                    if set_type != 'train':\n",
    "                        if file.split('_')[3] in train_bearing_codes:\n",
    "                            continue\n",
    "                        feature_array = np.load(f'{data_dir}/{file}', allow_pickle = True).astype('float32').ravel()\n",
    "                    else:\n",
    "                        feature_array = np.load(f'{testing_data_dir}/{file}', allow_pickle = True).astype('float32').ravel()\n",
    "                    feature_array = fft_transform(feature_array,\n",
    "                              zero_padding_multiple_ = zero_padding_multiple,\n",
    "                              num_coefs_to_save_ = NUM_COEF_TO_SAVE * zero_padding_multiple)\n",
    "                    arrays.append(feature_array)\n",
    "\n",
    "                for i in range(len(arrays)):\n",
    "                    arrays[i] = np.reshape(arrays[i], (1, arrays[i].shape[0]))\n",
    "                X = np.concatenate(arrays, axis = 0); del arrays\n",
    "                scaler = StandardScaler()\n",
    "                X = scaler.fit_transform(X)\n",
    "                np.save(f'{write_data_dir}/x_{set_type}_{regime}_{levels}_{bin_width}.npy', X); del X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def read_npy(file_path_):\n",
    "    return np.load(file_path_, allow_pickle = True)\n",
    "\n",
    "for REGIME in ['N09_M07_F10', 'N15_M07_F10', 'N15_M01_F10', 'N15_M07_F04']:\n",
    "    for fft_bin_width, fft_zero_padding_multiple in [(15, 1)]:\n",
    "\n",
    "        fft_file_suffix = f'{REGIME}_{fft_zero_padding_multiple}_{fft_bin_width}.npy'\n",
    "        fft_sets = dict(\n",
    "            x_train = read_npy(f'{write_data_dir}/x_train_{fft_file_suffix}'),\n",
    "            x_test = read_npy(f'{write_data_dir}/x_test_{fft_file_suffix}'),\n",
    "            x_val = read_npy(f'{write_data_dir}/x_val_{fft_file_suffix}'),\n",
    "            y_train = read_npy(f'{write_data_dir}/y_train_{fft_file_suffix}'),\n",
    "            y_test = read_npy(f'{write_data_dir}/y_test_{fft_file_suffix}'),\n",
    "            y_val = read_npy(f'{write_data_dir}/y_val_{fft_file_suffix}'),\n",
    "        )\n",
    "\n",
    "        for wavelet_levels, wavelet_bin_width in [(3,700)]:\n",
    "\n",
    "            wavelet_file_suffix = f'{REGIME}_{wavelet_levels}_{wavelet_bin_width}.npy'\n",
    "            wavelet_sets = dict(\n",
    "                x_train = read_npy(f'{write_data_dir}/x_train_{wavelet_file_suffix}'),\n",
    "                x_test = read_npy(f'{write_data_dir}/x_test_{wavelet_file_suffix}'),\n",
    "                x_val = read_npy(f'{write_data_dir}/x_val_{wavelet_file_suffix}'),\n",
    "                y_train = read_npy(f'{write_data_dir}/y_train_{wavelet_file_suffix}'),\n",
    "                y_test = read_npy(f'{write_data_dir}/y_test_{wavelet_file_suffix}'),\n",
    "                y_val = read_npy(f'{write_data_dir}/y_val_{wavelet_file_suffix}'),\n",
    "            )\n",
    "\n",
    "            combined_id = f'fft_{fft_bin_width}_{fft_zero_padding_multiple}_' \\\n",
    "                          f'wavelet_{wavelet_bin_width}_{wavelet_levels}'\n",
    "\n",
    "            for set_type in ['train', 'test', 'val']:\n",
    "\n",
    "                assert fft_sets[f'y_{set_type}'].tolist() == fft_sets[f'y_{set_type}'].tolist(), f'diff in  y_{set_type}'\n",
    "\n",
    "                # features\n",
    "                x_set = np.concatenate([fft_sets[f'x_{set_type}'], wavelet_sets[f'x_{set_type}']], axis = 1)\n",
    "\n",
    "                if set_type == 'train':\n",
    "\n",
    "                    scaler = StandardScaler()\n",
    "                    x_set_scaled = scaler.fit_transform(x_set); del x_set\n",
    "                    np.save(f'{write_data_dir}/x_{set_type}_{REGIME}_{combined_id}.npy', x_set_scaled)\n",
    "                    del x_set_scaled\n",
    "\n",
    "                if set_type == 'test':\n",
    "\n",
    "                    x_set_scaled = scaler.transform(x_set); del x_set\n",
    "                    np.save(f'{write_data_dir}/x_{set_type}_{REGIME}_{combined_id}.npy', x_set_scaled)\n",
    "                    del x_set_scaled\n",
    "\n",
    "                if set_type == 'val':\n",
    "\n",
    "                    x_set_scaled = scaler.transform(x_set); del x_set\n",
    "                    np.save(f'{write_data_dir}/x_{set_type}_{REGIME}_{combined_id}.npy', x_set_scaled)\n",
    "                    del x_set_scaled\n",
    "\n",
    "                # target\n",
    "                np.save(f'{write_data_dir}/y_{set_type}_{REGIME}_{combined_id}.npy', fft_sets[f'y_{set_type}'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}